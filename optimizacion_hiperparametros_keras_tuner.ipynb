{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34e0ce59",
   "metadata": {},
   "source": [
    "📚 Introducción Teórica\n",
    "¿Qué son los Hiperparámetros?\n",
    "Los hiperparámetros son configuraciones que definen la arquitectura y el comportamiento de un modelo de aprendizaje automático, pero que no se aprenden durante el entrenamiento. A diferencia de los parámetros (como pesos y sesgos), los hiperparámetros deben ser establecidos antes del entrenamiento.\n",
    "\n",
    "🔍 Diferencias Clave: Parámetros vs Hiperparámetros\n",
    "Aspecto\tParámetros\tHiperparámetros\n",
    "Definición\tVariables aprendidas por el modelo\tConfiguraciones establecidas antes del entrenamiento\n",
    "Ejemplos\tPesos, sesgos\tLearning rate, número de capas, dropout rate\n",
    "Optimización\tGradient descent, backpropagation\tGrid search, random search, Bayesian optimization\n",
    "Modificación\tDurante el entrenamiento\tAntes del entrenamiento\n",
    "\n",
    "⚠️ Importancia de la Optimización de Hiperparámetros\n",
    "Rendimiento: Puede mejorar la precisión del modelo en 5-15%\n",
    "Generalización: Reduce overfitting y mejora la capacidad de generalización\n",
    "Eficiencia: Optimiza el tiempo de entrenamiento y los recursos computacionales\n",
    "Robustez: Hace el modelo más estable ante variaciones en los datos\n",
    "\n",
    "🛠️ Métodos Tradicionales vs Keras Tuner\n",
    "Métodos Tradicionales:\n",
    "\n",
    "Manual: Ajuste basado en experiencia e intuición\n",
    "Grid Search: Búsqueda exhaustiva en una grilla predefinida\n",
    "Random Search: Selección aleatoria de combinaciones\n",
    "\n",
    "Ventajas de Keras Tuner:\n",
    "\n",
    "🔧 Facilidad de uso: API simple y consistente\n",
    "🚀 Algoritmos avanzados: Hyperband, Bayesian Optimization\n",
    "📊 Integración nativa: Funciona perfectamente con Keras/TensorFlow\n",
    "💾 Persistencia automática: Guarda resultados y permite reanudar búsquedas\n",
    "📈 Visualización: Herramientas integradas para análisis de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4810c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Instalación de Keras Tuner y dependencias\n",
    "!pip install -q keras_tuner\n",
    "!pip install -q seaborn\n",
    "print(\"✅ Instalación completada exitosamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21cc4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 Importación de librerías esenciales\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Librerías de sklearn para datos y preprocesamiento\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Configuración para reproducibilidad\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configuración de matplotlib para mejores gráficos\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"🎯 Librerías importadas correctamente\")\n",
    "print(f\"📊 TensorFlow version: {tf.__version__}\")\n",
    "print(f\"🔧 Keras Tuner version: {kt.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa54e6bc",
   "metadata": {},
   "source": [
    "📊 Preparación y Análisis del Dataset\n",
    "Utilizaremos el Breast Cancer Wisconsin Dataset, un dataset clásico para clasificación binaria que contiene características extraídas de imágenes digitalizadas de masas de tejido mamario.\n",
    "\n",
    "🔬 Características del Dataset\n",
    "Instancias: 569 muestras\n",
    "Features: 30 características numéricas\n",
    "Clases: Maligno (1) y Benigno (0)\n",
    "Tipo: Problema de clasificación binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf5a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📥 Carga y exploración del dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "print(\"🔍 ANÁLISIS EXPLORATORIO DEL DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"📈 Forma del dataset: {X.shape}\")\n",
    "print(f\"🎯 Clases: {data.target_names}\")\n",
    "print(f\"📊 Distribución de clases: {np.bincount(y)}\")\n",
    "print(f\"📋 Características: {len(data.feature_names)}\")\n",
    "\n",
    "# Crear DataFrame para mejor visualización\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "print(\"\\n📋 ESTADÍSTICAS DESCRIPTIVAS:\")\n",
    "print(df.describe().round(2))\n",
    "\n",
    "print(f\"\\n🎯 BALANCE DE CLASES:\")\n",
    "print(f\"Benigno (0): {(y == 1).sum()} ({(y == 1).mean()*100:.1f}%)\")\n",
    "print(f\"Maligno (1): {(y == 0).sum()} ({(y == 0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c27462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Visualización del dataset\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Distribución de clases\n",
    "axes[0, 0].pie([212, 357], labels=['Maligno', 'Benigno'], autopct='%1.1f%%', colors=['#ff6b6b', '#4ecdc4'])\n",
    "axes[0, 0].set_title('🎯 Distribución de Clases')\n",
    "\n",
    "# Histograma de algunas características importantes\n",
    "axes[0, 1].hist([df[df['target']==0]['mean radius'], df[df['target']==1]['mean radius']], alpha=0.7, label=['Maligno', 'Benigno'], bins=20)\n",
    "axes[0, 1].set_title('📏 Distribución del Radio Medio')\n",
    "axes[0, 1].set_xlabel('Radio Medio')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Correlación entre algunas características\n",
    "correlation_features = ['mean radius', 'mean texture', 'mean perimeter', 'mean area']\n",
    "corr_matrix = df[correlation_features + ['target']].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('🔥 Mapa de Correlación')\n",
    "\n",
    "# Boxplot de características importantes\n",
    "df_melted = df[['mean radius', 'mean texture', 'target']].melt(id_vars=['target'])\n",
    "sns.boxplot(data=df_melted, x='variable', y='value', hue='target', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('📦 Distribución por Clase')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Visualización del dataset completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35665dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 División y preprocesamiento de datos\n",
    "print(\"🔄 PREPROCESAMIENTO DE DATOS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# División estratificada del dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"📊 Conjunto de entrenamiento: {X_train.shape}\")\n",
    "print(f\"🧪 Conjunto de prueba: {X_test.shape}\")\n",
    "\n",
    "# Verificar distribución en conjuntos\n",
    "print(f\"\\n🎯 Distribución en entrenamiento:\")\n",
    "print(f\" Benigno: {(y_train == 1).sum()} ({(y_train == 1).mean()*100:.1f}%)\")\n",
    "print(f\" Maligno: {(y_train == 0).sum()} ({(y_train == 0).mean()*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n🎯 Distribución en prueba:\")\n",
    "print(f\" Benigno: {(y_test == 1).sum()} ({(y_test == 1).mean()*100:.1f}%)\")\n",
    "print(f\" Maligno: {(y_test == 0).sum()} ({(y_test == 0).mean()*100:.1f}%)\")\n",
    "\n",
    "# Estandarización de características\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n📐 Estadísticas después de la estandarización:\")\n",
    "print(f\" Media del conjunto de entrenamiento: {X_train_scaled.mean():.3f}\")\n",
    "print(f\" Desviación estándar del entrenamiento: {X_train_scaled.std():.3f}\")\n",
    "\n",
    "# Visualizar el efecto de la estandarización\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.hist(X_train[:, 0], bins=30, alpha=0.7, label='Original')\n",
    "ax1.set_title('📊 Antes de Estandarización')\n",
    "ax1.set_xlabel('Valores')\n",
    "ax1.set_ylabel('Frecuencia')\n",
    "ax2.hist(X_train_scaled[:, 0], bins=30, alpha=0.7, label='Estandarizado', color='orange')\n",
    "ax2.set_title('📊 Después de Estandarización')\n",
    "ax2.set_xlabel('Valores')\n",
    "ax2.set_ylabel('Frecuencia')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Preprocesamiento completado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72771500",
   "metadata": {},
   "source": [
    "🏗️ Función de Construcción del Modelo Base\n",
    "Definiremos una función que construye modelos con arquitectura variable, permitiendo ajustar múltiples hiperparámetros simultáneamente.\n",
    "\n",
    "💡 Hiperparámetros a Optimizar\n",
    "Arquitectura: Número de capas ocultas (1-5)\n",
    "Neuronas: Unidades por capa (32-512)\n",
    "Activación: Funciones (ReLU, Tanh, Sigmoid)\n",
    "Regularización: L2 regularization (1e-5 to 1e-2)\n",
    "Dropout: Tasa de dropout (0.0-0.5)\n",
    "Optimizador: Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af32788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    \"\"\" 🏗️ Construye un modelo de red neuronal con hiperparámetros variables\n",
    "    \n",
    "    Args:\n",
    "        hp: Objeto HyperParameters de Keras Tuner\n",
    "    \n",
    "    Returns:\n",
    "        model: Modelo compilado de Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # 🧱 Inicializar modelo secuencial\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # 📏 Definir número de capas ocultas\n",
    "    num_layers = hp.Int(\n",
    "        name='num_layers',\n",
    "        min_value=1,\n",
    "        max_value=5,\n",
    "        default=2\n",
    "    )\n",
    "    \n",
    "    # 🎯 Primera capa (incluye input_shape)\n",
    "    model.add(layers.Dense(\n",
    "        units=hp.Int(\n",
    "            name='units_0',\n",
    "            min_value=32,\n",
    "            max_value=512,\n",
    "            step=32,\n",
    "            default=128\n",
    "        ),\n",
    "        activation=hp.Choice(\n",
    "            name='activation_0',\n",
    "            values=['relu', 'tanh', 'sigmoid'],\n",
    "            default='relu'\n",
    "        ),\n",
    "        kernel_regularizer=keras.regularizers.l2(\n",
    "            hp.Float(\n",
    "                name='l2_0',\n",
    "                min_value=1e-5,\n",
    "                max_value=1e-2,\n",
    "                sampling='log',\n",
    "                default=1e-4\n",
    "            )\n",
    "        ),\n",
    "        input_shape=(X_train_scaled.shape[1],)\n",
    "    ))\n",
    "    \n",
    "    # ➕ Capas ocultas adicionales\n",
    "    for i in range(1, num_layers):\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(\n",
    "                name=f'units_{i}',\n",
    "                min_value=32,\n",
    "                max_value=512,\n",
    "                step=32,\n",
    "                default=64\n",
    "            ),\n",
    "            activation=hp.Choice(\n",
    "                name=f'activation_{i}',\n",
    "                values=['relu', 'tanh', 'sigmoid'],\n",
    "                default='relu'\n",
    "            ),\n",
    "            kernel_regularizer=keras.regularizers.l2(\n",
    "                hp.Float(\n",
    "                    name=f'l2_{i}',\n",
    "                    min_value=1e-5,\n",
    "                    max_value=1e-2,\n",
    "                    sampling='log',\n",
    "                    default=1e-4\n",
    "                )\n",
    "            )\n",
    "        ))\n",
    "        \n",
    "        # 🚫 Agregar dropout entre capas\n",
    "        model.add(layers.Dropout(\n",
    "            rate=hp.Float(\n",
    "                name=f'dropout_{i}',\n",
    "                min_value=0.0,\n",
    "                max_value=0.5,\n",
    "                step=0.1,\n",
    "                default=0.2\n",
    "            )\n",
    "        ))\n",
    "    \n",
    "    # 🎯 Capa de salida para clasificación binaria\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # ⚙️ Seleccionar optimizador\n",
    "    optimizer_choice = hp.Choice(\n",
    "        name='optimizer',\n",
    "        values=['adam', 'sgd', 'rmsprop'],\n",
    "        default='adam'\n",
    "    )\n",
    "    \n",
    "    # 📐 Compilar modelo\n",
    "    model.compile(\n",
    "        optimizer=optimizer_choice,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 🧪 Prueba de la función\n",
    "print(\"🧪 PRUEBA DE LA FUNCIÓN BUILD_MODEL\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Crear un objeto de hiperparámetros de prueba\n",
    "test_hp = kt.HyperParameters()\n",
    "test_model = build_model(test_hp)\n",
    "\n",
    "print(\"✅ Función build_model creada exitosamente\")\n",
    "print(f\"📊 Modelo de prueba creado con arquitectura:\")\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a16ee2",
   "metadata": {},
   "source": [
    "🚀 EJERCICIO 1: Investigación e Implementación de Hyperband\n",
    "📚 Teoría: Algoritmo Hyperband\n",
    "Hyperband es un algoritmo de optimización de hiperparámetros basado en el problema de \"multi-armed bandit\" que utiliza early stopping de manera principiada.\n",
    "\n",
    "🎯 Principios Fundamentales\n",
    "Hyperband se basa en el algoritmo Successive Halving:\n",
    "\n",
    "R: Presupuesto máximo de recursos\n",
    "η: Factor de reducción (típicamente 3 o 4)\n",
    "r_i: Recursos asignados en la iteración i\n",
    "🔄 Proceso de Optimización\n",
    "Inicialización: Se generan n configuraciones aleatorias\n",
    "Evaluación: Cada configuración se entrena con R/η^k recursos\n",
    "Selección: Se mantienen las mejores η configuraciones\n",
    "Iteración: Se repite el proceso aumentando los recursos\n",
    "⚡ Ventajas Principales\n",
    "Eficiencia Computacional: Elimina configuraciones pobres rápidamente\n",
    "No requiere conocimiento previo: No necesita configuración manual\n",
    "Balanceo automático: Equilibra exploración vs explotación\n",
    "Escalabilidad: Funciona bien con espacios grandes de hiperparámetros\n",
    "⚠️ Consideraciones Importantes\n",
    "Funciona mejor cuando hay correlación entre rendimiento temprano y final\n",
    "Puede no ser óptimo para modelos que requieren muchas épocas para converger\n",
    "El factor η debe ajustarse según el problema específico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c7159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 Implementación de Hyperband\n",
    "print(\"🚀 CONFIGURANDO HYPERBAND TUNER\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Configurar Hyperband tuner\n",
    "hyperband_tuner = kt.Hyperband(\n",
    "    hypermodel=build_model,  # Función que construye el modelo\n",
    "    objective='val_accuracy',  # Métrica a optimizar\n",
    "    max_epochs=50,  # Número máximo de épocas\n",
    "    factor=3,  # Factor de reducción η\n",
    "    hyperband_iterations=2,  # Número de iteraciones de Hyperband\n",
    "    directory='hyperband_results',  # Directorio para guardar resultados\n",
    "    project_name='breast_cancer_hyperband',  # Nombre del proyecto\n",
    "    overwrite=True  # Sobrescribir resultados anteriores\n",
    ")\n",
    "\n",
    "# Mostrar información del tuner\n",
    "print(f\"📊 Objetivo de optimización: {hyperband_tuner.objective.name}\")\n",
    "print(f\"🔧 Factor de reducción: {hyperband_tuner.factor}\")\n",
    "print(f\"⏱️ Épocas máximas: {hyperband_tuner.max_epochs}\")\n",
    "print(f\"🔄 Iteraciones de Hyperband: {hyperband_tuner.hyperband_iterations}\")\n",
    "\n",
    "# Configurar callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"✅ Hyperband tuner configurado exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb72af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 Ejecutar búsqueda con Hyperband\n",
    "print(\"🔍 EJECUTANDO BÚSQUEDA HYPERBAND\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Ejecutar la búsqueda\n",
    "hyperband_tuner.search(\n",
    "    x=X_train_scaled,\n",
    "    y=y_train,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "hyperband_duration = end_time - start_time\n",
    "\n",
    "print(f\"\\n⏱️ Tiempo total de búsqueda: {hyperband_duration:.2f} segundos\")\n",
    "print(\"✅ Búsqueda Hyperband completada exitosamente\")\n",
    "\n",
    "# Obtener los mejores hiperparámetros\n",
    "best_hps_hyperband = hyperband_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\\n🏆 MEJORES HIPERPARÁMETROS ENCONTRADOS POR HYPERBAND:\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"📊 Número de capas: {best_hps_hyperband.get('num_layers')}\")\n",
    "print(f\"⚙️ Optimizador: {best_hps_hyperband.get('optimizer')}\")\n",
    "\n",
    "for i in range(best_hps_hyperband.get('num_layers')):\n",
    "    print(f\"🔸 Capa {i+1}:\")\n",
    "    print(f\" • Unidades: {best_hps_hyperband.get(f'units_{i}')}\")\n",
    "    print(f\" • Activación: {best_hps_hyperband.get(f'activation_{i}')}\")\n",
    "    print(f\" • L2 regularization: {best_hps_hyperband.get(f'l2_{i}'):.2e}\")\n",
    "    if i > 0:\n",
    "        print(f\" • Dropout: {best_hps_hyperband.get(f'dropout_{i}')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5dcc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Análisis de resultados de Hyperband\n",
    "print(\"📊 ANÁLISIS DE RESULTADOS - HYPERBAND\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "# Obtener todos los trials\n",
    "hyperband_trials = hyperband_tuner.oracle.get_best_trials(num_trials=10)\n",
    "\n",
    "# Crear visualización de resultados\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Evolución de scores\n",
    "trial_ids = [trial.trial_id for trial in hyperband_trials]\n",
    "scores = [trial.score if trial.score is not None else 0 for trial in hyperband_trials]\n",
    "axes[0, 0].plot(trial_ids, scores, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_title('🚀 Hyperband: Evolución de Scores')\n",
    "axes[0, 0].set_xlabel('Trial ID')\n",
    "axes[0, 0].set_ylabel('Validation Accuracy')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_ylim(0.85, 1.0)\n",
    "\n",
    "# 2. Distribución de scores\n",
    "axes[0, 1].hist(scores, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 1].axvline(max(scores), color='red', linestyle='--', label=f'Mejor: {max(scores):.4f}')\n",
    "axes[0, 1].set_title('📊 Distribución de Accuracy')\n",
    "axes[0, 1].set_xlabel('Validation Accuracy')\n",
    "axes[0, 1].set_ylabel('Frecuencia')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Análisis de número de capas\n",
    "num_layers_list = []\n",
    "scores_by_layers = []\n",
    "for trial in hyperband_trials:\n",
    "    if trial.score is not None:\n",
    "        num_layers_list.append(trial.hyperparameters.get('num_layers'))\n",
    "        scores_by_layers.append(trial.score)\n",
    "axes[1, 0].scatter(num_layers_list, scores_by_layers, alpha=0.7, s=100, c='orange')\n",
    "axes[1, 0].set_title('🏗️ Número de Capas vs Accuracy')\n",
    "axes[1, 0].set_xlabel('Número de Capas')\n",
    "axes[1, 0].set_ylabel('Validation Accuracy')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_xticks(range(1, 6))\n",
    "\n",
    "# 4. Análisis de optimizadores\n",
    "optimizers_list = []\n",
    "for trial in hyperband_trials:\n",
    "    if trial.score is not None:\n",
    "        optimizers_list.append(trial.hyperparameters.get('optimizer'))\n",
    "from collections import Counter\n",
    "opt_counts = Counter(optimizers_list)\n",
    "axes[1, 1].bar(opt_counts.keys(), opt_counts.values(), color=['#ff9999', '#66b3ff', '#99ff99'])\n",
    "axes[1, 1].set_title('⚙️ Distribución de Optimizadores (Top 10)')\n",
    "axes[1, 1].set_ylabel('Frecuencia')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estadísticas de rendimiento\n",
    "print(f\"\\n📈 ESTADÍSTICAS DE RENDIMIENTO:\")\n",
    "print(f\" • Mejor accuracy: {max(scores):.4f}\")\n",
    "print(f\" • Accuracy promedio: {np.mean(scores):.4f}\")\n",
    "print(f\" • Desviación estándar: {np.std(scores):.4f}\")\n",
    "print(f\" • Número de trials exitosos: {len([s for s in scores if s > 0])}\")\n",
    "\n",
    "print(f\"\\n🏗️ ANÁLISIS ARQUITECTURAL:\")\n",
    "layers_performance = {}\n",
    "for layers, score in zip(num_layers_list, scores_by_layers):\n",
    "    if layers not in layers_performance:\n",
    "        layers_performance[layers] = []\n",
    "    layers_performance[layers].append(score)\n",
    "for layers in sorted(layers_performance.keys()):\n",
    "    scores_layer = layers_performance[layers]\n",
    "    print(f\" • {layers} capas: Promedio = {np.mean(scores_layer):.4f}, \"\n",
    "          f\"Mejor = {max(scores_layer):.4f} ({len(scores_layer)} trials)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fb77e3",
   "metadata": {},
   "source": [
    "🧠 EJERCICIO 2: Investigación e Implementación de Optimización Bayesiana\n",
    "📚 Teoría: Optimización Bayesiana\n",
    "La Optimización Bayesiana es una técnica de optimización global que utiliza modelos probabilísticos para encontrar el óptimo de funciones costosas de evaluar.\n",
    "\n",
    "🧮 Componentes Fundamentales\n",
    "1. Modelo Sustituto (Gaussian Process)\n",
    "Un Proceso Gaussiano (GP) modela la función objetivo desconocida f(x):\n",
    "\n",
    "μ(x): Función media (típicamente 0)\n",
    "k(x, x'): Función de covarianza (kernel)\n",
    "2. Función de Adquisición\n",
    "Determina qué punto evaluar siguiente balanceando exploración vs explotación:\n",
    "\n",
    "f⁺: Mejor valor observado hasta ahora\n",
    "μ(x), σ(x): Media y desviación estándar del GP\n",
    "Φ, φ: CDF y PDF de la distribución normal estándar\n",
    "🔄 Proceso Iterativo\n",
    "Inicialización: Evaluar algunos puntos aleatorios\n",
    "Ajuste del GP: Entrenar el modelo sustituto\n",
    "Optimización de adquisición: Encontrar x* que maximiza la función de adquisición\n",
    "Evaluación: Evaluar f(x*) y agregar a los datos\n",
    "Repetir: Hasta alcanzar el presupuesto o convergencia\n",
    "✅ Ventajas sobre Métodos Tradicionales\n",
    "Eficiencia: Requiere menos evaluaciones para encontrar el óptimo\n",
    "Principiada: Usa información de evaluaciones previas de manera óptima\n",
    "Incertidumbre: Cuantifica la confianza en las predicciones\n",
    "Balance automático: Equilibra exploración y explotación naturalmente\n",
    "💡 Kernels Comunes en GP\n",
    "RBF (Radial Basis Function): k(x,x') = σ²exp(-||x-x'||²/2l²)\n",
    "Matérn: Para funciones menos suaves\n",
    "Linear: Para relaciones lineales\n",
    "Periodic: Para patrones periódicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ee9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧠 Implementación de Optimización Bayesiana\n",
    "print(\"🧠 CONFIGURANDO BAYESIAN OPTIMIZATION TUNER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configurar Bayesian Optimization tuner\n",
    "bayesian_tuner = kt.BayesianOptimization(\n",
    "    hypermodel=build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=25,  # Número de trials (menor que random search)\n",
    "    num_initial_points=5,  # Puntos de exploración inicial\n",
    "    alpha=1e-4,  # Parámetro de regularización del GP\n",
    "    beta=2.6,  # Parámetro de exploración (UCB)\n",
    "    directory='bayesian_results',\n",
    "    project_name='breast_cancer_bayesian',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(f\"📊 Objetivo de optimización: {bayesian_tuner.objective.name}\")\n",
    "print(f\"🔬 Máximo de trials: {bayesian_tuner.max_trials}\")\n",
    "print(f\"🎯 Puntos iniciales: {bayesian_tuner.num_initial_points}\")\n",
    "print(f\"🔧 Alpha (regularización): {bayesian_tuner.alpha}\")\n",
    "print(f\"🎛️ Beta (exploración): {bayesian_tuner.beta}\")\n",
    "\n",
    "print(\"✅ Bayesian Optimization tuner configurado exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab530acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 Ejecutar búsqueda con Optimización Bayesiana\n",
    "print(\"🔍 EJECUTANDO OPTIMIZACIÓN BAYESIANA\")\n",
    "print(\"=\" * 42)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Ejecutar la búsqueda\n",
    "bayesian_tuner.search(\n",
    "    x=X_train_scaled,\n",
    "    y=y_train,\n",
    "    epochs=40,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "bayesian_duration = end_time - start_time\n",
    "\n",
    "print(f\"\\n⏱️ Tiempo total de búsqueda: {bayesian_duration:.2f} segundos\")\n",
    "print(\"✅ Optimización Bayesiana completada exitosamente\")\n",
    "\n",
    "# Obtener los mejores hiperparámetros\n",
    "best_hps_bayesian = bayesian_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\\n🏆 MEJORES HIPERPARÁMETROS - OPTIMIZACIÓN BAYESIANA:\")\n",
    "print(\"=\" * 58)\n",
    "print(f\"📊 Número de capas: {best_hps_bayesian.get('num_layers')}\")\n",
    "print(f\"⚙️ Optimizador: {best_hps_bayesian.get('optimizer')}\")\n",
    "\n",
    "for i in range(best_hps_bayesian.get('num_layers')):\n",
    "    print(f\"🔸 Capa {i+1}:\")\n",
    "    print(f\" • Unidades: {best_hps_bayesian.get(f'units_{i}')}\")\n",
    "    print(f\" • Activación: {best_hps_bayesian.get(f'activation_{i}')}\")\n",
    "    print(f\" • L2 regularization: {best_hps_bayesian.get(f'l2_{i}'):.2e}\")\n",
    "    if i > 0:\n",
    "        print(f\" • Dropout: {best_hps_bayesian.get(f'dropout_{i}')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Comparación entre Hyperband y Optimización Bayesiana\n",
    "print(\"📊 COMPARACIÓN DE MÉTODOS DE OPTIMIZACIÓN\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Obtener trials de ambos métodos\n",
    "bayesian_trials = bayesian_tuner.oracle.get_best_trials(num_trials=15)\n",
    "\n",
    "# Preparar datos para comparación\n",
    "hyperband_scores = [trial.score for trial in hyperband_trials if trial.score is not None]\n",
    "bayesian_scores = [trial.score for trial in bayesian_trials if trial.score is not None]\n",
    "\n",
    "# Crear visualización comparativa\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Comparación de distribuciones\n",
    "axes[0, 0].hist(hyperband_scores, bins=10, alpha=0.7, label='Hyperband', color='lightblue')\n",
    "axes[0, 0].hist(bayesian_scores, bins=10, alpha=0.7, label='Bayesian Opt.', color='lightcoral')\n",
    "axes[0, 0].set_title('📊 Distribución de Scores')\n",
    "axes[0, 0].set_xlabel('Validation Accuracy')\n",
    "axes[0, 0].set_ylabel('Frecuencia')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box plots comparativo\n",
    "data_comparison = [hyperband_scores, bayesian_scores]\n",
    "axes[0, 1].boxplot(data_comparison, labels=['Hyperband', 'Bayesian Opt.'])\n",
    "axes[0, 1].set_title('📦 Comparación de Rendimiento')\n",
    "axes[0, 1].set_ylabel('Validation Accuracy')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Evolución temporal (simulada)\n",
    "trials_hyperband = list(range(1, len(hyperband_scores) + 1))\n",
    "trials_bayesian = list(range(1, len(bayesian_scores) + 1))\n",
    "axes[0, 2].plot(trials_hyperband, hyperband_scores, 'o-', label='Hyperband', linewidth=2)\n",
    "axes[0, 2].plot(trials_bayesian, bayesian_scores, 's-', label='Bayesian Opt.', linewidth=2)\n",
    "axes[0, 2].set_title('⏱️ Evolución de Scores')\n",
    "axes[0, 2].set_xlabel('Trial Number')\n",
    "axes[0, 2].set_ylabel('Validation Accuracy')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Estadísticas de rendimiento\n",
    "methods = ['Hyperband', 'Bayesian Opt.']\n",
    "best_scores = [max(hyperband_scores), max(bayesian_scores)]\n",
    "mean_scores = [np.mean(hyperband_scores), np.mean(bayesian_scores)]\n",
    "std_scores = [np.std(hyperband_scores), np.std(bayesian_scores)]\n",
    "x_pos = np.arange(len(methods))\n",
    "axes[1, 0].bar(x_pos - 0.2, best_scores, 0.4, label='Mejor Score', alpha=0.8)\n",
    "axes[1, 0].bar(x_pos + 0.2, mean_scores, 0.4, label='Score Promedio', alpha=0.8)\n",
    "axes[1, 0].set_title('🏆 Comparación de Rendimiento')\n",
    "axes[1, 0].set_ylabel('Validation Accuracy')\n",
    "axes[1, 0].set_xticks(x_pos)\n",
    "axes[1, 0].set_xticklabels(methods)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Eficiencia temporal\n",
    "durations = [hyperband_duration, bayesian_duration]\n",
    "efficiency = [best_scores[i] / (durations[i] / 60) for i in range(2)]  # Score por minuto\n",
    "axes[1, 1].bar(methods, durations, color=['lightblue', 'lightcoral'], alpha=0.7)\n",
    "axes[1, 1].set_title('⏱️ Tiempo de Ejecución')\n",
    "axes[1, 1].set_ylabel('Tiempo (segundos)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Eficiencia (Score/Tiempo)\n",
    "axes[1, 2].bar(methods, efficiency, color=['navy', 'darkred'], alpha=0.7)\n",
    "axes[1, 2].set_title('⚡ Eficiencia (Score/Minuto)')\n",
    "axes[1, 2].set_ylabel('Eficiencia')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Imprimir estadísticas detalladas\n",
    "print(f\"\\n📈 ESTADÍSTICAS COMPARATIVAS:\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"🚀 HYPERBAND:\")\n",
    "print(f\" • Mejor accuracy: {max(hyperband_scores):.4f}\")\n",
    "print(f\" • Accuracy promedio: {np.mean(hyperband_scores):.4f} ± {np.std(hyperband_scores):.4f}\")\n",
    "print(f\" • Tiempo total: {hyperband_duration:.1f} segundos\")\n",
    "print(f\" • Trials exitosos: {len(hyperband_scores)}\")\n",
    "print(f\"\\n🧠 OPTIMIZACIÓN BAYESIANA:\")\n",
    "print(f\" • Mejor accuracy: {max(bayesian_scores):.4f}\")\n",
    "print(f\" • Accuracy promedio: {np.mean(bayesian_scores):.4f} ± {np.std(bayesian_scores):.4f}\")\n",
    "print(f\" • Tiempo total: {bayesian_duration:.1f} segundos\")\n",
    "print(f\" • Trials exitosos: {len(bayesian_scores)}\")\n",
    "print(f\"\\n⚡ ANÁLISIS DE EFICIENCIA:\")\n",
    "print(f\" • Hyperband: {efficiency[0]:.6f} score/minuto\")\n",
    "print(f\" • Bayesian Opt.: {efficiency[1]:.6f} score/minuto\")\n",
    "winner = \"Hyperband\" if max(hyperband_scores) > max(bayesian_scores) else \"Optimización Bayesiana\"\n",
    "print(f\"\\n🏆 Ganador en accuracy: {winner}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc153a7",
   "metadata": {},
   "source": [
    "📈 EJERCICIO 3: Visualización Avanzada de Resultados\n",
    "La visualización de resultados es crucial para entender el comportamiento de los algoritmos de optimización y tomar decisiones informadas sobre la selección de hiperparámetros.\n",
    "\n",
    "🎨 Importancia de la Visualización en Optimización\n",
    "Convergencia: Observar cómo mejoran los algoritmos con el tiempo\n",
    "Exploración vs Explotación: Entender el balance de los algoritmos\n",
    "Identificación de patrones: Detectar relaciones entre hiperparámetros\n",
    "Validación de resultados: Confirmar la calidad de la optimización\n",
    "Comunicación: Presentar resultados de manera clara"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
