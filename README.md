# Optimizaci贸n de Hiperpar谩metros con Keras Tuner

Este proyecto implementa un an谩lisis completo de optimizaci贸n de hiperpar谩metros para un modelo de clasificaci贸n binaria utilizando el dataset Breast Cancer Wisconsin. Se comparan dos algoritmos avanzados de optimizaci贸n: Hyperband y Optimizaci贸n Bayesiana.

##  Contenido del Notebook

### Introducci贸n Te贸rica
- Conceptos fundamentales de hiperpar谩metros
- Diferencias entre par谩metros e hiperpar谩metros
- Importancia de la optimizaci贸n de hiperpar谩metros
- Comparaci贸n entre m茅todos tradicionales y Keras Tuner

### Preparaci贸n y An谩lisis del Dataset
- Carga y exploraci贸n del Breast Cancer Wisconsin dataset
- An谩lisis exploratorio de datos (EDA)
- Visualizaciones del dataset
- Preprocesamiento y estandarizaci贸n

### Funci贸n de Construcci贸n del Modelo
- Arquitectura variable de red neuronal
- Hiperpar谩metros optimizables:
  - N煤mero de capas ocultas (1-5)
  - Unidades por capa (32-512)
  - Funciones de activaci贸n (ReLU, Tanh, Sigmoid)
  - Regularizaci贸n L2
  - Tasa de dropout
  - Optimizador (Adam, SGD, RMSprop)

### Ejercicio 1: Implementaci贸n de Hyperband
- Teor铆a del algoritmo Hyperband
- Configuraci贸n y ejecuci贸n de la b煤squeda
- An谩lisis de resultados y visualizaciones

### Ejercicio 2: Implementaci贸n de Optimizaci贸n Bayesiana
- Teor铆a de la optimizaci贸n bayesiana
- Componentes fundamentales (Gaussian Process, Funci贸n de Adquisici贸n)
- Configuraci贸n y ejecuci贸n de la b煤squeda
- An谩lisis comparativo con Hyperband

### Ejercicio 3: Visualizaci贸n Avanzada de Resultados
- Importancia de la visualizaci贸n en optimizaci贸n
- Gr谩ficos comparativos de rendimiento
- An谩lisis de eficiencia temporal
- Interpretaci贸n de resultados

##  Requisitos del Sistema

- Python 3.7+
- TensorFlow 2.x
- Keras Tuner
- Scikit-learn
- NumPy, Pandas, Matplotlib, Seaborn

##  Instalaci贸n

```bash
pip install -r requirements.txt
```

##  Uso

1. Abrir el notebook `optimizacion_hiperparametros_keras_tuner.ipynb`
2. Ejecutar las celdas en orden
3. Los resultados de la optimizaci贸n se guardan autom谩ticamente en directorios separados

##  Resultados Esperados

- Comparaci贸n de rendimiento entre Hyperband y Optimizaci贸n Bayesiana
- Mejores hiperpar谩metros encontrados para el dataset
- Visualizaciones detalladas del proceso de optimizaci贸n
- An谩lisis de eficiencia computacional

##  Configuraci贸n de la Optimizaci贸n

### Hyperband
- M谩ximo 50 茅pocas
- Factor de reducci贸n: 3
- 2 iteraciones de Hyperband

### Optimizaci贸n Bayesiana
- M谩ximo 25 trials
- 5 puntos iniciales de exploraci贸n
- Par谩metros GP: alpha=1e-4, beta=2.6

##  M茅tricas de Evaluaci贸n

- Accuracy de validaci贸n
- Precision y Recall
- Tiempo de ejecuci贸n
- Eficiencia (score por minuto)

##  Contribuciones

Este proyecto sigue las mejores pr谩cticas modernas de machine learning y est谩 dise帽ado para fines educativos y de investigaci贸n.

##  Licencia

Este proyecto es de uso educativo.